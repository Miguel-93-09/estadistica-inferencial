% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
  ignorenonframetext,
]{beamer}
\usepackage{pgfpages}
\setbeamertemplate{caption}[numbered]
\setbeamertemplate{caption label separator}{: }
\setbeamercolor{caption name}{fg=normal text.fg}
\beamertemplatenavigationsymbolsempty
% Prevent slide breaks in the middle of a paragraph
\widowpenalties 1 10000
\raggedbottom
\setbeamertemplate{part page}{
  \centering
  \begin{beamercolorbox}[sep=16pt,center]{part title}
    \usebeamerfont{part title}\insertpart\par
  \end{beamercolorbox}
}
\setbeamertemplate{section page}{
  \centering
  \begin{beamercolorbox}[sep=12pt,center]{section title}
    \usebeamerfont{section title}\insertsection\par
  \end{beamercolorbox}
}
\setbeamertemplate{subsection page}{
  \centering
  \begin{beamercolorbox}[sep=8pt,center]{subsection title}
    \usebeamerfont{subsection title}\insertsubsection\par
  \end{beamercolorbox}
}
\AtBeginPart{
  \frame{\partpage}
}
\AtBeginSection{
  \ifbibliography
  \else
    \frame{\sectionpage}
  \fi
}
\AtBeginSubsection{
  \frame{\subsectionpage}
}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\usecolortheme{dolphin}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\newif\ifbibliography
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Tema 2 - Estimación Puntual},
  pdfauthor={Ricardo Alberich, Juan Gabriel Gomila y Arnau Mir},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Tema 2 - Estimación Puntual}
\author{Ricardo Alberich, Juan Gabriel Gomila y Arnau Mir}
\date{}

\begin{document}
\frame{\titlepage}

\section{Definiciones básicas}\label{definiciones-buxe1sicas}

\begin{frame}{Estadística inferencial}
\phantomsection\label{estaduxedstica-inferencial}
El problema usual de la \textbf{estadística inferencial} es:

\begin{itemize}
\item
  Queremos conocer el valor de una característica en una población
\item
  No podemos medir esta característica en todos los individuos de la
  población
\item
  Extraemos una muestra aleatoria de la población, medimos la
  característica en los individuos de esta muestra e \textbf{inferimos}
  el valor de la característica para la toda la población

  \begin{itemize}
  \tightlist
  \item
    ¿Cómo lo tenemos que hacer?
  \item
    ¿Cómo tenemos que hacer la muestra?
  \item
    ¿Qué información podemos inferir?
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Idea intuitiva de muestra aleatoria simple}
\phantomsection\label{idea-intuitiva-de-muestra-aleatoria-simple}
Muestra aleatoria simple (m.a.s.) de tamaño \(n\): de una población de
\(N\) individuos, repetimos \(n\) veces el proceso consistente en
escoger \textbf{equiprobablemente} un individuo de la población;
\emph{los individuos escogidos se pueden repetir}

\textbf{Ejemplo}

Escogemos al azar \(n\) estudiantes de la Universidad de las Islas
Baleares (UIB) (con reposición) para medirles la estatura

De esta manera, todas las muestras posibles de \(n\) individuos
(posiblemente repetidos: \emph{multiconjuntos}) tienen la misma
probabilidad
\end{frame}

\begin{frame}{Idea intuitiva de estadístico}
\phantomsection\label{idea-intuitiva-de-estaduxedstico}
Estadístico (\emph{Estimador puntual}): una función que aplicada a una
muestra nos permite \emph{estimar} un valor que queramos conocer sobre
toda la población.

\textbf{Ejemplo}

La media de las estaturas de una muestra de estudiantes de la UIB nos
permite estimar la media de las alturas de todos los estudiantes de la
UIB.
\end{frame}

\begin{frame}{Definición formal de muestra aleatoria simple}
\phantomsection\label{definiciuxf3n-formal-de-muestra-aleatoria-simple}
Una m.a.s. de tamaño \(n\) (de una v.a. \(X\)) es

\begin{itemize}
\item
  un conjunto de \(n\) copias independientes de \(X\), o
\item
  un conjunto de \(n\) variables aleatorias independientes
  \(X_1,\ldots,X_n\), todas con la distribución de \(X\).
\end{itemize}

\textbf{Ejemplo}

Sea \(X\) la v.a. ``escogemos un estudiante de la UIB y le medimos la
altura''. Una m.a.s. de \(X\) de tamaño \(n\) serán \(n\) copias
independientes \(X_1,\ldots,X_n\) de esta \(X\).

Una realización de una m.a.s. son los \(n\) valores \(x_1,\ldots,x_n\)
que toman las v.a. \(X_1,\ldots,X_n\).
\end{frame}

\begin{frame}{Definición formal de estadístico}
\phantomsection\label{definiciuxf3n-formal-de-estaduxedstico}
Un \emph{estadístico} \(T\) es una función aplicada a la muestra
\(X_1,\ldots,X_n\): \[
T=f(X_1,\ldots,X_n)
\] Este estadístico se aplica a las realizaciones de la muestra

Definición: la \textbf{media muestral} de una m.a.s. \(X_1,\ldots,X_n\)
de tamaño \(n\) es \[
\overline{X}:=\frac{X_1+\cdots+X_n}{n}
\] y estima \(E(X)\).

\textbf{Ejemplo:}

La \textbf{media muestral} de las alturas de una realización de una
m.a.s. de las alturas de estudiantes estima la altura media de un
estudiante de la UIB.
\end{frame}

\begin{frame}{Definición formal de estadístico}
\phantomsection\label{definiciuxf3n-formal-de-estaduxedstico-1}
Así pues, un \textbf{estadístico} es una (otra) variable aleatoria, con
distribución, esperanza, etc.

La \textbf{distribución muestral} de \(T\) es la distribución de esta
variable aleatoria.

Estudiando esta distribución muestral, podremos estimar propiedades de
\(X\) a partir del comportamiento de una muestra.

Error estándar de \(T\): desviación típica de \(T\).
\end{frame}

\begin{frame}{Convenio: LOS ESTADÍSTICOS, EN MAYÚSCULAS; las
realizaciones, en minúsculas}
\phantomsection\label{convenio-los-estaduxedsticos-en-mayuxfasculas-las-realizaciones-en-minuxfasculas}
\begin{itemize}
\item
  \(X_1,\ldots,X_n\) una m.a.s. y \[
  \overline{X}:=\frac{X_1+\cdots+X_n}{n},
  \] el estadístico media muestral.
\item
  \(x_1,\ldots,x_n\) una realización de esta m.a.s. y \[
  \overline{x}:=\frac{x_1+\cdots+x_n}{n},
  \] la media (muestral) de esta realización.
\end{itemize}
\end{frame}

\begin{frame}{En la vida real\ldots{}}
\phantomsection\label{en-la-vida-real}
En la vida real, las muestras aleatorias se toman, casi siempre, sin
reposición (es decir sin repetición del mismo individuo de la
población).

No son muestras aleatorias simples. pero:

\begin{itemize}
\item
  Si \(N\) es mucho más grande que \(n\), los resultados para una
  m.a.s.~son (aproximadamente) los mismos, ya que las repeticiones son
  improbables y las variables aleatorias que forman la muestra son
  prácticamente independientes.
\item
  En estos casos cometeremos el abuso de lenguaje de decir que es una
  m.a.s.
\item
  Si \(n\) es relativamente grande, se suelen dar versiones corregidas
  de los estadísticos.
\end{itemize}
\end{frame}

\section{La media muestral}\label{la-media-muestral}

\begin{frame}{Definición de media muestral}
\phantomsection\label{definiciuxf3n-de-media-muestral}
Media muestral : sea \(X_1,\ldots, X_n\) una m.a.s.~de tamaño \(n\) de
una v.a.~\(X\) de esperanza \(\mu_X\) y desviación típica \(\sigma_X\)

La \emph{media muestral} es: \[
\overline{X}=\frac{X_1+\cdots+X_n}{n}
\]

Proposición

En estas condiciones, \[
E(\overline{X})=\mu_X,\quad \sigma_{\overline{X}}=\frac{\sigma_X}{\sqrt{n}}
\] donde \(\sigma_{\overline{X}}\) es el \textbf{error estándar} de
\(\overline{X}\).
\end{frame}

\begin{frame}{Propiedades de la media muestral}
\phantomsection\label{propiedades-de-la-media-muestral}
Proposición

\begin{itemize}
\item
  Es un estimador puntual de \(\mu_X\)
\item
  \(E(\overline{X})=\mu_X\): el valor esperado de \(\overline{X}\) es
  \(\mu_X\).
\item
  Si tomamos muchas veces una m.a.s. y calculamos la media muestral, el
  valor medio de estas medias tiende con mucha probabilidad a ser
  \(\mu_X\).
\item
  \(\sigma_{\overline{X}}= \sigma_X/\sqrt{n}\): la variabilidad de los
  resultados de \(\overline{X}\) tiende a 0 a medida que tomamos
  muestras más grandes.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Media muestral. Ejemplo del \texttt{iris}}
\phantomsection\label{media-muestral.-ejemplo-del-iris}
\textbf{Ejercicio}

Consideremos la tabla de datos \texttt{iris} (que ya vimos en el tema de
\emph{Muestreo}). Vamos a comprobar las propiedades anteriores sobre la
variable \textbf{longitud del pétalo} (\texttt{Petal.Length}).

\begin{enumerate}
\tightlist
\item
  Generaremos 10000 muestras de tamaño 40 con reposición de las
  longitudes del pétalo.
\item
  A continuación hallaremos los valores medios de cada muestra.
\item
  Consideraremos la media y la desviación típica de dichos valores
  medios y los compararemos con los valores exactos dados por las
  propiedades de la media muestral.
\end{enumerate}
\end{frame}

\begin{frame}[fragile]{Media muestral. Ejemplo del \texttt{iris}}
\phantomsection\label{media-muestral.-ejemplo-del-iris-1}
Para generar los valores medios de las longitudes del pétalo de las
10000 muestras usaremos la función \texttt{replicate} de \texttt{R}.
Fijaos en su sintaxis:

\begin{itemize}
\tightlist
\item
  \texttt{replicate(n,expresión)} evalúa \texttt{n} veces la
  \texttt{expresión}, y organiza los resultados como las columnas de una
  matriz (o un vector, si el resultado de cada \texttt{expresión} es
  unidimensional).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1001}\NormalTok{)}
\FunctionTok{str}\NormalTok{(iris)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 'data.frame':    150 obs. of  5 variables:
##  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...
##  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...
##  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...
##  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...
##  $ Species     : Factor w/ 3 levels "setosa","versicolor",..: 1 1 1 1 1 1 1 1 1 1 ...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{valores.medios.long.pétalo}\OtherTok{=}\FunctionTok{replicate}\NormalTok{(}\DecValTok{10000}\NormalTok{,}\FunctionTok{mean}\NormalTok{(}\FunctionTok{sample}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Petal.Length,}\DecValTok{40}\NormalTok{,}
                                                      \AttributeTok{replace =}\ConstantTok{TRUE}\NormalTok{)))}
\end{Highlighting}
\end{Shaded}

Los valores medios de las 10 primeras muestras anteriores serían

\begin{verbatim}
##  [1] 3.5975 3.5150 3.9400 3.2650 3.9125 3.9650 4.2825 3.2950 3.8500 3.7850
\end{verbatim}

\begin{verbatim}
##  num [1:10000] 3.6 3.52 3.94 3.27 3.91 ...
\end{verbatim}
\end{frame}

\begin{frame}[fragile]{Media muestral. Ejemplo del \texttt{iris}}
\phantomsection\label{media-muestral.-ejemplo-del-iris-2}
El valor medio de los valores medios de las muestras anteriores vale:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(valores.medios.long.pétalo)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.754478
\end{verbatim}

Dicho valor tiene que estar cerca del valor medio de la variable
longitud del pétalo:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Petal.Length)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 3.758
\end{verbatim}

Fijaos que los dos valores están muy próximos.
\end{frame}

\begin{frame}[fragile]{Media muestral. Ejemplo del \texttt{iris}}
\phantomsection\label{media-muestral.-ejemplo-del-iris-3}
La desviación típica de los valores medios de las muestras vale:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{(valores.medios.long.pétalo)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2796513
\end{verbatim}

Dicho valor tiene que estar cerca de \(\frac{\sigma_{lp}}{\sqrt{40}}\)
(donde \(\sigma_{lp}\) es la desviación típica de la variable longitud
del pétalo) tal como predice la propiedad de la media muestral referida
a la desviación típica de la misma:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Petal.Length)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1.765298
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Petal.Length)}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{40}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.2791182
\end{verbatim}

Fijaos también en que los dos valores están muy próximos.
\end{frame}

\section{Poblaciones normales}\label{poblaciones-normales}

\begin{frame}{Combinación lineal de distribuciones normales}
\phantomsection\label{combinaciuxf3n-lineal-de-distribuciones-normales}
Proposición La combinación lineal de distribuciones normales es normal.
Es decir, si \(Y_1,\ldots,Y_n\) son v.a.~normales independientes, cada
\(Y_i\sim N(\mu_i,\sigma_i)\), y \(a_1,\ldots,a_n,b\in \mathbb{R}\)
entonces \[
Y=a_1Y_1+\cdots+a_nY_n+b
\] es una v.a.~\(N(\mu,\sigma)\) con \(\mu\) y \(\sigma\) las que
correspondan:

\begin{itemize}
\item
  \(E(Y)=a_1\cdot\mu_1+\cdots+a_n\cdot\mu_n+b\)
\item
  \(\sigma(Y)^2=a_1^2\cdot\sigma_1^2+\cdots+a_n^2\cdot\sigma_n^2\)
\end{itemize}
\end{frame}

\begin{frame}{Distribución de la media muestral}
\phantomsection\label{distribuciuxf3n-de-la-media-muestral}
Veamos cómo se distribuye la media muestral en el caso en que la
población \(X\) sea normal.

Proposición

Sea \(X_1,\ldots, X_n\) una m.a.s. de una v.a. \(X\) de esperanza
\(\mu_X\) y desviación típica \(\sigma_X\).

Si \(X\) es \(N(\mu_X,\sigma_X)\), entonces \[
\overline{X}\mbox{ es }N\Big(\mu_X,\frac{\sigma_X}{\sqrt{n}}\Big)
\] y por lo tanto \[
Z=\frac{\overline{X}-\mu_X}{\frac{\sigma_X}{\sqrt{n}}}\mbox{ es }N(0,1)
\]

\(Z\) es la \textbf{expresión tipificada} de la media muestral.
\end{frame}

\begin{frame}{Teorema Central del Límite}
\phantomsection\label{teorema-central-del-luxedmite}
Teorema Central del Límite. Sea \(X_1,\ldots, X_n\) una m.a.s. de una
v.a. \(X\) \textbf{cualquiera} de esperanza \(\mu_X\) y desviación
típica \(\sigma_X\). Cuando \(n\to \infty\), \[
\overline{X}\to N\Big(\mu_X,\frac{\sigma_X}{\sqrt{n}}\Big)
\] y por lo tanto \[
Z=\frac{\overline{X}-\mu_X}{\frac{\sigma_X}{\sqrt{n}}}\to N(0,1)
\] (estas convergencias se refieren a las distribuciones.)
\end{frame}

\begin{frame}{Teorema Central del Límite}
\phantomsection\label{teorema-central-del-luxedmite-1}
Caso \(n\) grande: Si \(n\) es grande (\textbf{\(n\geq 30\) o 40}),
\(\overline{X}\) es aproximadamente normal, con esperanza \(\mu_X\) y
desviación típica \(\dfrac{\sigma_X}{\sqrt{n}}\)

\textbf{Ejemplo}

Tenemos una v.a. \(X\) de media \(\mu_X=3\) y desviación típica.
\(\sigma_X=0.2\). Tomamos muestras aleatorias simples de tamaño 50. La
distribución de la media muestral \(\overline{X}\) es aproximadamente \[
N\left(3,\frac{0.2}{\sqrt{50}}\right)=N(3,0.0283).
\]
\end{frame}

\begin{frame}{Teorema Central del Límite}
\phantomsection\label{teorema-central-del-luxedmite-2}
En el gráfico siguiente podemos observar el histograma de los valores
medios de las longitudes del pétalo de las 10000 muestras junto con la
distribución normal correspondiente:

\begin{center}\includegraphics{Tema-2---Estimacion_files/figure-beamer/unnamed-chunk-7-1} \end{center}
\end{frame}

\begin{frame}{Ejemplo}
\phantomsection\label{ejemplo}
\textbf{Ejercicio}

El tamaño en megabytes (MB) de un tipo de imágenes comprimidas tiene un
valor medio de \(115\) MB, con una desviación típica de \(25\). Tomamos
una m.a.s. de \(100\) imágenes de este tipo.

¿Cuál es la probabilidad de que la media muestral del tamaño de los
ficheros sea \(\leq 110\) MB?

Sea \(X\) la variable aleatoria que nos da el tamaño en megabytes del
tipo de imágenes comprimidas. La distribución de \(X\) será
\(X=N(\mu=115,\sigma = 25)\)

Sea \(X_1,\ldots,X_{100}\) la m.a.s. La distribución aproximada de la
media muestral \(\overline{X}\) usando el \textbf{Teorema Central del
Límite} será:
\(\overline{X}\approx N\left(\mu_{\overline{X}}=115,\sigma_{\overline{X}}=\frac{25}{\sqrt{100}}=2.5\right)\).

Nos piden la probabilidad siguiente: \(P(\overline{X}\leq 110)\). Si
estandarizamos:

\[
P(\overline{X}\leq 110)=P\left(Z=\frac{\overline{X}-115}{2.5}\leq \frac{110-115}{2.5}\right) =p(Z\leq -2)=0.0228.
\] donde \(Z\) es la normal estándar \(N(0,1)\)
\end{frame}

\begin{frame}{Media muestral en muestras sin reposición}
\phantomsection\label{media-muestral-en-muestras-sin-reposiciuxf3n}
Sea \(X_1,\ldots, X_n\) una m.a. \textbf{sin reposición} de tamaño \(n\)
de una v.a. \(X\) de esperanza \(\mu_X\) y desviación típica
\(\sigma_X\).

Si \(n\) es pequeño en relación al tamaño \(N\) de la población, todo lo
que hemos contado funciona (aproximadamente).

Si \(n\) es grande en relación a \(N\), entonces \[
E(\overline{X})=\mu_X,\quad \sigma_{\overline{X}}=\frac{\sigma_X}{\sqrt{n}}\cdot \sqrt{\frac{N-n}{N-1}}
\] (\textbf{factor de población finita})

El Teorema Central del Límite ya no funciona exactamente en este último
caso.
\end{frame}

\section{Proporción muestral}\label{proporciuxf3n-muestral}

\begin{frame}{Proporción muestral. Definición}
\phantomsection\label{proporciuxf3n-muestral.-definiciuxf3n}
Proporción muestral. Sea \(X\) una v.a. Bernoulli de parámetro \(p_X\)
(1 éxito, 0 fracaso). Sea \(X_1,\ldots,X_n\) una m.a.s. de tamaño \(n\)
de \(X\).

\(S=\sum_{i=1}^n X_i\) es el nombre de éxitos observados es \(B(n,p)\).

La \textbf{proporción muestral} es \[
\widehat{p}_X=\frac{S}{n}
\] y es un estimador de \(p_X\).

Notemos que \(\widehat{p}_X\) es un caso particular de \(\overline{X}\),
por lo que todo lo que hemos dicho para medias muestrales es cierto para
proporciones muestrales.
\end{frame}

\begin{frame}{Proporción muestral. Propiedades}
\phantomsection\label{proporciuxf3n-muestral.-propiedades}
Proposición

\begin{itemize}
\item
  Valor esperado de la proporción muestral: \[E(\widehat{p}_X)=p_X\]
\item
  \textbf{Error estándar} de la proporción muestral:
\end{itemize}

\[\displaystyle \sigma_{\widehat{p}_X}=\sqrt{\frac{p_X(1-p_X)}{n}}\]

\begin{itemize}
\tightlist
\item
  Si la muestra es sin reposición y \(n\) es relativamente grande,
\end{itemize}

\[\displaystyle\sigma_{\widehat{p}_X}=\sqrt{\frac{p_X(1-p_X)}{n}}\cdot{\sqrt{\frac{N-n}{N-1}}}.\]
\end{frame}

\begin{frame}{Proporción muestral. Propiedades}
\phantomsection\label{proporciuxf3n-muestral.-propiedades-1}
Teorema: Si \(n\) es grande (\(n\geq 30\) o 40) y la muestra es
aleatoria simple, usando el Teorema Central del Límite, \[
\frac{\widehat{p}_X-p_X}{\sqrt{\frac{{p}_X(1-{p}_X)}{n}}}\approx N(0,1)
\]
\end{frame}

\begin{frame}[fragile]{Proporción muestral. Ejemplo del \texttt{iris}}
\phantomsection\label{proporciuxf3n-muestral.-ejemplo-del-iris}
\textbf{Ejercicio}

Dada una muestra de 60 flores de la tabla de datos \texttt{iris},

\begin{enumerate}
\tightlist
\item
  Estimar la proporción de flores de la especie \texttt{setosa}.
\item
  Estimar también la desviación estándar de dicha proporción.
\end{enumerate}

Primero generamos la muestra de las 60 flores:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1000}\NormalTok{)}
\NormalTok{flores.elegidas }\OtherTok{=} \FunctionTok{sample}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{150}\NormalTok{,}\DecValTok{60}\NormalTok{,}\AttributeTok{replace=}\ConstantTok{TRUE}\NormalTok{)}
\NormalTok{muestra.flores }\OtherTok{=}\NormalTok{ iris[flores.elegidas,]}
\end{Highlighting}
\end{Shaded}
\end{frame}

\begin{frame}[fragile]{Proporción muestral. Ejemplo del \texttt{iris}}
\phantomsection\label{proporciuxf3n-muestral.-ejemplo-del-iris-1}
A continuación miramos cuántas flores de la muestra son de la especie
\texttt{setosa}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{table}\NormalTok{(muestra.flores}\SpecialCharTok{$}\NormalTok{Species}\SpecialCharTok{==}\StringTok{"setosa"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## FALSE  TRUE 
##    39    21
\end{verbatim}

Tenemos entonces 21 flores de la especie \texttt{setosa}.
\end{frame}

\begin{frame}[fragile]{Proporción muestral. Ejemplo del \texttt{iris}}
\phantomsection\label{proporciuxf3n-muestral.-ejemplo-del-iris-2}
La estimación de la proporción de flores de especie \texttt{setosa}
será:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\AttributeTok{prop.setosa =} \FunctionTok{table}\NormalTok{(muestra.flores}\SpecialCharTok{$}\NormalTok{Species}\SpecialCharTok{==}\StringTok{"setosa"}\NormalTok{)[}\DecValTok{2}\NormalTok{]}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(muestra.flores}\SpecialCharTok{$}\NormalTok{Species))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## TRUE 
## 0.35
\end{verbatim}

valor que no está muy lejos del valor poblacional de la proporción
\(p_{setosa}\) que es \(p_{setosa}=\frac{50}{150}=0.3333\).

Para estimar la desviación estándar de la proporción muestral de flores
de tamaño 60 de la especie \texttt{setosa}, repetiremos el experimento
anterior 10000 veces y hallaremos la desviación estándar de las
proporciones obtenidas. Al final, compararemos dicho valor con el valor
exacto dado por la propiedad correspondiente.
\end{frame}

\begin{frame}[fragile]{Proporción muestral. Ejemplo del \texttt{iris}}
\phantomsection\label{proporciuxf3n-muestral.-ejemplo-del-iris-3}
Para generar las proporciones de las 10000 muestras usaremos la función
\texttt{replicate} de \texttt{R}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1002}\NormalTok{)}
\NormalTok{props.muestrales }\OtherTok{=} \FunctionTok{replicate}\NormalTok{(}\DecValTok{10000}\NormalTok{,}\FunctionTok{table}\NormalTok{(}\FunctionTok{sample}\NormalTok{(iris}\SpecialCharTok{$}\NormalTok{Species,}\DecValTok{60}\NormalTok{,}
                              \AttributeTok{replace=}\ConstantTok{TRUE}\NormalTok{)}\SpecialCharTok{==}\StringTok{"setosa"}\NormalTok{)[}\DecValTok{2}\NormalTok{]}\SpecialCharTok{/}\DecValTok{60}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

La desviación típica de las proporciones muestrales anteriores vale:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{mean}\NormalTok{(props.muestrales)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.3333233
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{(props.muestrales)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.06021098
\end{verbatim}

valor muy próximo al valor real que vale:
\(\displaystyle \sigma_{\widehat{p}_X}=\sqrt{\frac{p_X(1-p_X)}{n}}= \sqrt{\frac{\frac{50}{150}\cdot \left(1-\frac{50}{150}\right)}{60}}=0.0609\).
\end{frame}

\begin{frame}{Proporción muestral. Ejemplo del \texttt{iris}}
\phantomsection\label{proporciuxf3n-muestral.-ejemplo-del-iris-4}
En el gráfico siguiente podemos observar el histograma de las
proporciones muestrales de las 10000 muestras junto con la distribución
normal correspondiente:

\begin{center}\includegraphics{Tema-2---Estimacion_files/figure-beamer/unnamed-chunk-13-1} \end{center}
\end{frame}

\section{Varianza muestral y desviación típica
muestral}\label{varianza-muestral-y-desviaciuxf3n-tuxedpica-muestral}

\begin{frame}{Varianza muestral y desviación típica muestral.
Definición}
\phantomsection\label{varianza-muestral-y-desviaciuxf3n-tuxedpica-muestral.-definiciuxf3n}
Varianza muestral y desviación típica muestral. Sea \(X_1,\ldots, X_n\)
una m.a.s. de tamaño \(n\) de una v.a. \(X\) de esperanza \(\mu_X\) y
desviación típica \(\sigma_X\).

La \textbf{varianza muestral} es \[
\widetilde{S}_{X}^2=\frac{\sum\limits_{i=1}^n (X_{i}-\overline{X})^2}{n-1}
\] La \textbf{desviación típica muestral} es \[
\widetilde{S}_{X}=+\sqrt{\widetilde{S}_{X}^2}
\]
\end{frame}

\begin{frame}{Varianza muestral y desviación típica muestral.
Definición}
\phantomsection\label{varianza-muestral-y-desviaciuxf3n-tuxedpica-muestral.-definiciuxf3n-1}
Además, escribiremos \[
S^2_{X}=\frac{\sum\limits_{i=1}^n (X_{i}-\overline{X})^2}{n}=\frac{(n-1)}{n}\widetilde{S}^2_{X}\quad\mbox{ y }\quad S_X=+\sqrt{S_X^2}
\]
\end{frame}

\begin{frame}{Varianza muestral y desviación típica muestral.
Propiedades}
\phantomsection\label{varianza-muestral-y-desviaciuxf3n-tuxedpica-muestral.-propiedades}
Propiedades
\[\displaystyle S^2_X=\frac{\sum\limits_{i=1}^n (X_{i}-\overline{X})^2}{n}=\left(\frac{\sum\limits_{i=1}^n
X_{i}^2}{n}-\overline{X}^2\right)\]

\[\displaystyle \widetilde{S}_{X}^2=\frac{n}{n-1}\left(\frac{\sum\limits_{i=1}^n
X_{i}^2}{n}-\overline{X}^2\right)\]
\end{frame}

\begin{frame}{Varianza muestral y desviación típica muestral.
Propiedades}
\phantomsection\label{varianza-muestral-y-desviaciuxf3n-tuxedpica-muestral.-propiedades-1}
Teorema. Si la v.a. \(X\) es normal, entonces
\(E(\widetilde{S}_{X}^2)=\sigma_{X}^2\) y la v.a. \[
\frac{(n-1)\widetilde{S}_{X}^2}{\sigma_{X}^2}
\] tiene distribución \(\chi_{n-1}^2\).
\end{frame}

\begin{frame}{La distribución \(\chi_{n-1}^2\)}
\phantomsection\label{la-distribuciuxf3n-chi_n-12}
Distribución \(\chi_n^2\)

La distribución \(\chi_n^2\) (\(\chi\): en catalán, \textbf{khi}; en
castellano, \textbf{ji}; en inglés, \textbf{chi}), donde \(n\) es un
parámetro llamado \textbf{grados de libertad}:

es la de \[
X=Z_{1}^{2}+Z_{2}^{2}+\cdots +Z_{n}^{2}
\] donde \(Z_{1},Z_{2},\ldots, Z_{n}\) son v.a. independientes
\(N(0,1)\).
\end{frame}

\begin{frame}{La distribución \(\chi_{n-1}^2\). Propiedades}
\phantomsection\label{la-distribuciuxf3n-chi_n-12.-propiedades}
Propiedades \(\chi_n^2\)

\begin{itemize}
\item
  Su función de densidad es \[
  f_{\chi_n^2}(x)={\frac{1}{2^{n/2} \Gamma (n/2)}} x^{(n/2)-1} e^{-x/2},\quad\mbox{ si $x\geq 0$}
  \] donde \(\Gamma(x)=\int_{0}^{\infty} t^{x-1}e^{-t}\, dt\), si
  \(x> 0\).
\item
  Si \(X_{\chi_n^2}\) es una v.a.~con distribución \(\chi_n^2\),
  \[E\left(X_{\chi_n^2}\right)=n,\quad Var\left(X_{\chi_n^2}\right)=2 n\]
\item
  \({\chi_n^2}\) se aproxima a una distribución normal
  \(N\left(n,\sqrt{2n}\right)\) para \(n\) grande (\(n>40\) o \(50\)).
\end{itemize}
\end{frame}

\begin{frame}[fragile]{La distribución \(\chi_{n-1}^2\). Gráficos}
\phantomsection\label{la-distribuciuxf3n-chi_n-12.-gruxe1ficos}
El gráfico de la función de densidad de distintas distribuciones
\(\chi^2_n\) para \(n=1,2,3,4,5,10\) se puede observar en el gráfico
siguiente:

\pandocbounded{\includegraphics[keepaspectratio]{Tema-2---Estimacion_files/figure-beamer/unnamed-chunk-14-1.pdf}}

\begin{frame}{La distribución \(\chi_{n-1}^2\). Ejemplo}
\phantomsection\label{la-distribuciuxf3n-chi_n-12.-ejemplo}
\textbf{Ejercicio}

Supongamos que el aumento diario del la ocupación de una granja de
discos duros medido en Gigas sigue una distribución normal con
desviación típica \(1.7\). Se toma una muestras de 12 discos. Supongamos
que esta muestra es pequeña respecto del total de la población de la
granja de discos.

¿Cual es la probabilidad de que la desviación típica muestral sea
\(\leq 2.5\)?

Sea \(X\)= aumento diario en Gigas de un disco duro elegido al azar.

Sabemos que \(\sigma_{X}^2=(1.7)^2=2.89.\)

Como que \(X\) es normal y \(n=12\), tenemos que \[
\frac{11\cdot \widetilde{S}_{X}^2}{2.89}=\frac{(n-1)\widetilde{S}_{X}^2}{\sigma_{X}^2}\sim \chi^2_{11}
\]
\end{frame}

\begin{frame}{La distribución \(\chi_{n-1}^2\). Ejemplo}
\phantomsection\label{la-distribuciuxf3n-chi_n-12.-ejemplo-1}
Nos piden:
\(\displaystyle P(\widetilde{S}_{X}<2.5)= P\left(\widetilde{S}_{X}^2<2.5^2\right)\):
\[
P\left(\widetilde{S}_{X}^2<2.5^2\right)  =  P\left(\frac{11\cdot \widetilde{S}_{X}^2}{2.89}<\frac{11
\cdot 2.5^2}{2.89}\right)  =  P(\chi_{11}^2<23.7889) = 0.9863.
\]
\end{frame}

\section{Propiedades de los
estimadores}\label{propiedades-de-los-estimadores}

\begin{frame}{Estimadores insesgados}
\phantomsection\label{estimadores-insesgados}
¿Cuándo un estimador es bueno?

Estimadores insesgados Un estimador puntual \(\widehat{\theta}\) de un
parámetro poblacional \(\theta\) es \textbf{insesgado, no sesgado o sin
sesgo} cuando su valor esperado es precisamente el valor del parámetro:
\[
E(\widehat{\theta})=\theta
\] Entonces se dice que el estimador puntual es \textbf{no sesgado}.

El \textbf{sesgo} de \(\widehat{\theta}\) es la diferencia
\[E(\widehat{\theta})-\theta\]
\end{frame}

\begin{frame}{Estimadores insesgados. Ejemplos}
\phantomsection\label{estimadores-insesgados.-ejemplos}
Proposición

\begin{itemize}
\item
  \(\overline{X}\) es estimador no sesgado de \(\mu_X\):
  \(E(\overline{X})=\mu_X\).
\item
  \(\widehat{p}_X\) es estimador no sesgado de \(p_X\):
  \(E(\widehat{p}_X)=p_X\).
\item
  Si \(X\) es normal: \(\widetilde{S}_{X}^2\) es estimador no sesgado de
  \(\sigma_X^2\): \(E(\widetilde{S}_{X}^2)=\sigma_X^2\)
\item
  Si \(X\) es normal: \(E({S}_{X}^2)=\dfrac{n-1}{n}\sigma_X^2\). Por lo
  tanto \({S}_{X}^2\), es sesgado, con sesgo \[
  E({S}_{X}^2)-\sigma_X^2=\dfrac{n-1}{n}\sigma_X^2-\sigma_X^2=-\dfrac{\sigma_X^2}{n}\mbox{ que tiende a  }0.
  \]
\end{itemize}
\end{frame}

\begin{frame}{Estimadores eficientes}
\phantomsection\label{estimadores-eficientes}
¿Cuando un estimador es \textbf{bueno}?

Cuando es no segado y tiene poca variabilidad (así es más probable que
aplicado a una m.a.s. dé un valor más cercano al valor esperado)

Error estándar de un estimador \(\widehat{\theta}\): es su desviación
típica \[\sigma_{\widehat{\theta}}=\sqrt{Var(\widehat{\theta})}\]
\end{frame}

\begin{frame}{Estimadores eficientes}
\phantomsection\label{estimadores-eficientes-1}
Eficiencia de un estimador Dados dos estimadores \(\widehat{\theta}_1\),
\(\widehat{\theta}_2\) no sesgados (o con sesgo que tiende a \(0\)) del
mismo parámetro \(\theta\), diremos que \(\widehat{\theta}_1\) es
\textbf{más eficiente} que \(\widehat{\theta}_2\) cuando

\[\sigma_{\widehat{\theta}_1}< \sigma_{\widehat{\theta}_2},\] es decir,
cuando \[Var(\widehat{\theta}_1)< Var(\widehat{\theta}_2).\]
\end{frame}

\begin{frame}{Estimadores eficientes. Ejemplo}
\phantomsection\label{estimadores-eficientes.-ejemplo}
Sea \(X\) una v.a. con media \(\mu_X\) y desviación típica \(\sigma_X\)

Consideremos la mediana \(Me=Q_{0.5}\) de la realización de una m.a.s.
de \(X\) como estimador puntual de \(\mu_X\)

Si \(X\) es normal, \[
\begin{array}{l}
E(Me)=\mu_X,\\
\displaystyle  Var(Me)\approx \frac{\pi}{2}
     \frac{\sigma_{X}^2}{n}\approx \frac{1.57 \sigma_{X}^2}{n}=1.57\cdot Var(\overline{X}) > Var(\overline{X}).
\end{array}
\] Por lo tanto, \(Me\) es un estimador no sesgado de \(\mu_X\), pero
menos eficiente que \(\overline{X}\).
\end{frame}

\begin{frame}{Estimadores eficientes}
\phantomsection\label{estimadores-eficientes-2}
Proposición

\begin{itemize}
\item
  Si la población es normal, la \textbf{media muestral} \(\bar X\) es el
  estimador no sesgado más eficiente de la \textbf{media poblacional}
  \(\mu_X\).
\item
  Si la población es Bernoulli, la \textbf{proporción muestral}
  \(\hat p_X\) es el estimador no sesgado más eficiente de la
  \textbf{proporción poblacional} \(p_X\).
\item
  Si la población es normal, la \textbf{varianza muestral}
  \(\tilde S^2_X\) es el estimador no sesgado más eficiente de la
  \textbf{varianza poblacional} \(\sigma_x^2\).
\end{itemize}
\end{frame}

\begin{frame}{Estimadores eficientes. ¿\(S_X^2\) o
\(\widetilde{S}_X^2\)?}
\phantomsection\label{estimadores-eficientes.-s_x2-o-widetildes_x2}
Como hemos visto, si la población es normal, la varianza muestral es el
estimador no sesgado más eficiente de la varianza poblacional

El estimador \emph{varianza} \[
S_X^2=\frac{(n-1)}{n} \widetilde{S}_X^2
\]\\
aunque sea más eficiente, tiene sesgo que tiende a \(0\).

Si \(n\) es pequeño (\(\leq 30\) o 40), es mejor utilizar la varianza
muestral \(\widetilde{S}_X^2\) para estimar la varianza, ya que el sesgo
influye, pero si \(n\) es grande, el sesgo ya no es tan importante y se
puede utilizar \(S_X^2\).
\end{frame}

\begin{frame}{Estimadores eficientes. Ejemplo}
\phantomsection\label{estimadores-eficientes.-ejemplo-1}
Tenemos una población numerada \(1,2,\ldots,N\)

Tomamos una m.a.s. \(x_1,\ldots,x_n\); sea \(m=\max\{x_1,\ldots,x_n\}\).

Teorema. El estimador no segado más eficiente del tamaño de la población
\(N\) es \[
\widehat{N}=m+\frac{m-n}{n}.
\] O sea, la manera más eficiente de estimar el número de elementos de
la población a partir de una muestra es usar la fórmula anterior.
\end{frame}

\begin{frame}[fragile]{Estimadores eficientes. Ejemplo}
\phantomsection\label{estimadores-eficientes.-ejemplo-2}
\textbf{Ejemplo}

Sentados en una terraza de un bar del Paseo Marítimo de Palma hemos
anotado el número de licencia de los 40 primeros taxis que hemos visto
pasar:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{taxis}\OtherTok{=}\FunctionTok{c}\NormalTok{(}\DecValTok{1217}\NormalTok{,}\DecValTok{600}\NormalTok{,}\DecValTok{883}\NormalTok{,}\DecValTok{1026}\NormalTok{,}\DecValTok{150}\NormalTok{,}\DecValTok{715}\NormalTok{,}\DecValTok{297}\NormalTok{,}\DecValTok{137}\NormalTok{,}\DecValTok{508}\NormalTok{,}\DecValTok{134}\NormalTok{,}\DecValTok{38}\NormalTok{,}\DecValTok{961}\NormalTok{,}\DecValTok{538}\NormalTok{,}\DecValTok{1154}\NormalTok{,}
        \DecValTok{314}\NormalTok{,}\DecValTok{1121}\NormalTok{,}\DecValTok{823}\NormalTok{,}\DecValTok{158}\NormalTok{,}\DecValTok{940}\NormalTok{,}\DecValTok{99}\NormalTok{,}\DecValTok{977}\NormalTok{,}\DecValTok{286}\NormalTok{,}\DecValTok{1006}\NormalTok{,}\DecValTok{1207}\NormalTok{,}\DecValTok{264}\NormalTok{,}\DecValTok{1183}\NormalTok{,}\DecValTok{1120}\NormalTok{,}
        \DecValTok{498}\NormalTok{,}\DecValTok{606}\NormalTok{,}\DecValTok{566}\NormalTok{,}\DecValTok{1239}\NormalTok{,}\DecValTok{860}\NormalTok{,}\DecValTok{114}\NormalTok{,}\DecValTok{701}\NormalTok{,}\DecValTok{381}\NormalTok{,}\DecValTok{836}\NormalTok{,}\DecValTok{561}\NormalTok{,}\DecValTok{494}\NormalTok{,}\DecValTok{858}\NormalTok{,}\DecValTok{187}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Supondremos que estas observaciones son una m.a.s. de los taxis de
Palma. Vamos a estimar el número total de taxis.

Entonces, estimamos que el número de taxis de Palma es

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(}\AttributeTok{N=}\FunctionTok{max}\NormalTok{(taxis)}\SpecialCharTok{+}\NormalTok{(}\FunctionTok{max}\NormalTok{(taxis)}\SpecialCharTok{{-}}\FunctionTok{length}\NormalTok{(taxis))}\SpecialCharTok{/}\FunctionTok{length}\NormalTok{(taxis))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1268.975
\end{verbatim}

En realidad, hay 1246.
\end{frame}

\begin{frame}{Estimadores máximo verosímiles}
\phantomsection\label{estimadores-muxe1ximo-verosuxedmiles}
¿Cómo encontramos buenos estimadores?

Antes de explicar la metodología, necesitamos una definición previa:

Función de verosimilitud de la muestra. Sea \(X\) una v.a.
\textbf{discreta} con función de probabilidad \(f_X(x;\theta)\) que
depende de un parámetro desconocido \(\theta\).

Sea \(X_{1},\ldots X_{n}\) una m.a.s. de \(X\), y sea
\(x_1,x_2,\ldots,x_n\) una realización de esta muestra.

La \textbf{función de verosimilitud} de la muestra es la probabilidad
condicionada siguiente: \[
\begin{array}{ll}
{L(\theta|x_1,x_2,\ldots,x_n)} & := P(x_1,x_2,\ldots,x_n|\theta)=P(X_1=x_1)\cdots P(X_n=x_n) \\ & = f_X(x_1;\theta)\cdots f_X(x_n;\theta).
\end{array}
\]
\end{frame}

\begin{frame}{Estimadores máximo verosímiles}
\phantomsection\label{estimadores-muxe1ximo-verosuxedmiles-1}
Dada la función de verosimilitud \(L(\theta|x_1,\ldots,x_n)\) de la
muestra, indicaremos por \(\hat{\theta}(x_1,\ldots,x_n)\) el valor del
parámetro \(\theta\) en el que se alcanza el máximo de
\(L(\theta|x_1,\ldots,x_n)\). Será una función de \(x_1,\ldots,x_n\).

Estimador máximo verosímil. Un estimador \(\hat{\theta}\) de un
parámetro \(\theta\) es \textbf{máximo verosímil} (\textbf{MV}) cuando,
para cada m.a.s, la probabilidad de observarlo es máxima, cuando el
parámetro toma el valor del estimador aplicado a la muestra, es decir,
si la función de verosimilitud
\[L(\theta|x_1,x_2,\ldots,x_n)= P(x_1,x_2,\ldots,x_n|\theta)\] alcanza
su máximo.
\end{frame}

\begin{frame}{Estimadores máximo verosímiles. Ejemplo}
\phantomsection\label{estimadores-muxe1ximo-verosuxedmiles.-ejemplo}
Supongamos que tenemos una v.a. Bernoulli \(X\) de probabilidad de éxito
\(p\) desconocida.

Para cada m.a.s. \(x_1,\ldots,x_n\) de \(X\), sean \(\widehat{p}_x\) su
proporción muestral y \(P(x_1,\ldots,x_n\mid p)\) la probabilidad de
obtenerla cuando el verdadero valor del parámetro es \(p\).

Teorema. El valor de \(p\) para el que \(P(x_1,\ldots,x_n\mid p)\) es
máximo es \(\widehat{p}_x\).

Dicho en otras palabras, la proporción muestral es un estimador MV de
\(p\).

\textbf{Ejercicio}

Demostrar el teorema anterior.
\end{frame}

\begin{frame}{Estimadores máximo verosímiles. Ejemplo}
\phantomsection\label{estimadores-muxe1ximo-verosuxedmiles.-ejemplo-1}
En general, al ser \(\ln\) una función creciente, en lugar de maximizar
\(L(\theta|x_1,\ldots,x_n)\), maximizamos \[
\ln(L(\theta|x_1,\ldots,x_n))
\] que suele ser más simple (ya que transforma los productos en sumas, y
es más fácil derivar estas últimas).
\end{frame}

\begin{frame}{Estimadores máximo verosímiles. Ejemplo}
\phantomsection\label{estimadores-muxe1ximo-verosuxedmiles.-ejemplo-2}
Sea \(X_{1},\ldots X_{n}\) una m.a.s. de una v.a. Bernoulli \(X\) de
parámetro \(p\) (desconocido). Denotemos \(q=1-p\) \[
\begin{array}{c}
f_X(1;p)=P(X=1)=p,\quad 
f_X(0;p)=P(X=0)=q
\end{array}
\] es a decir, para \(x\in\{0,1\}\), resulta que
\(f_X(x;p)=P(X=x)=p^{x} q^{1-x}.\)

La función de verosimilitud es: \[
\begin{array}{l}
L(p|x_1,\ldots,x_n) = f_{X}(x_1;p)\cdots f_{X}(x_n;p) =
p^{x_{1}}q^{1-x_{1}} \cdots  p^{x_{n}}q^{1-x_{n}}
\\
= p^{\sum_{i=1}^n x_{i}} q^{\sum_{i=1}^n (1-x_{i})}= p^{\sum_{i=1}^n x_{i}} q^{n-\sum_{i=1}^n x_{i}}
 =p^{\sum_{i=1}^n x_{i}} (1-p)^{n-\sum_{i=1}^n x_{i}}
\end{array}
\]
\end{frame}

\begin{frame}{Estimadores máximo verosímiles. Ejemplo}
\phantomsection\label{estimadores-muxe1ximo-verosuxedmiles.-ejemplo-3}
La función de verosimilitud es \[
L(p|x_1,\ldots,x_n)  =p^{\sum_{i=1}^n x_{i}} (1-p)^{n-\sum_{i=1}^n x_{i}}=p^{n\overline{x}}(1-p)^{n-n\overline{x}},
\] donde \(\overline{x}=\dfrac{\sum_{i=1}^n x_{i}}{n}\).

Queremos encontrar el valor de \(p\) en el que se alcanza el máximo de
esta función (donde \(\overline{x}\) es un parámetro y la variable es
\(p\))

Maximizaremos su logaritmo: \[
\ln(L(p|x_1,\ldots,x_n))=n\overline{x}\ln(p)+n(1-\overline{x})\ln(1-p).
\]
\end{frame}

\begin{frame}{Estimadores máximo verosímiles. Ejemplo}
\phantomsection\label{estimadores-muxe1ximo-verosuxedmiles.-ejemplo-4}
Derivamos respecto de \(p\): \[
\begin{array}{l}
\ln(L(p|x_1,\ldots,x_n))' =n\overline{x}\frac{1}{p}-n(1-\overline{x})\frac{1}{1-p}\\
\qquad =\frac{1}{p(1-p)}\Big((1-p)n\overline{x}-pn(1-\overline{x})\Big)
 =\frac{1}{p(1-p)}(n\overline{x} -pn) \\ \qquad=\frac{n}{p(1-p)}(\overline{x} -p)
\end{array}
\] Estudiamos el signo: \[
\ln(L(p|x_1,\ldots,x_n))'\geq 0  \Leftrightarrow \overline{x} -p\geq 0 \Leftrightarrow
p\leq\overline{x}
\]
\end{frame}

\begin{frame}{Estimadores máximo verosímiles. Ejemplo}
\phantomsection\label{estimadores-muxe1ximo-verosuxedmiles.-ejemplo-5}
Por lo tanto \[
\ln(L(p|x_1,\ldots,x_n))\left\{
\begin{array}{l}
\mbox{ creciente hasta $\overline{x}$}\\
\mbox{ decreciente a partir de $\overline{x}$}\\
\mbox{ tiene un máximo en $\overline{x}$}
\end{array}\right.
\]

El resultado queda demostrado.
\(L(\widehat{p}_X|x_1,\ldots,x_n)\geq L(p|x_1,\ldots,x_n)\) para
cualquier \(p\).
\end{frame}

\begin{frame}{Algunos estimadores \textbf{MV}}
\phantomsection\label{algunos-estimadores-mv}
Proposición

\begin{itemize}
\item
  \(\widehat{p}_x\) es el estimador MV del parámetro \(p\) de una v.a.
  Bernoulli.
\item
  \(\overline{X}\) es el estimador MV del parámetro \(\theta\) de una
  v.a. Poisson.
\item
  \(\overline{X}\) es el estimador MV del parámetro \(\mu\) de una v.a.
  normal.
\item
  \(S_X^2\) (\textbf{no} \(\widetilde{S}_X^2\)) es el estimador MV del
  parámetro \(\sigma^2\) de una v.a. normal.
\item
  El máximo (\textbf{no} \(\widehat{N}\)) es el estimador MV de la \(N\)
  en el problema de los taxis.
\end{itemize}
\end{frame}

\begin{frame}{Estimadores máximo verosímiles. Ejemplo:
captura-recaptura}
\phantomsection\label{estimadores-muxe1ximo-verosuxedmiles.-ejemplo-captura-recaptura}
En una población hay \(N\) individuos, capturamos \(K\), los marcamos y
los volvemos a soltar.

Ahora volvemos a capturar \(n\), de los que \(k\) están marcados. A
partir de estos datos, queremos estimar \(N\).

Supongamos que \(N\) y \(K\) no han cambiado de la primera a la segunda
captura.

La variable aleatoria \(X=\) \emph{Un individuo esté marcado} es
\(Be(p)\) con \(p=\dfrac{K}{N}\).

Si \(X_1,\ldots,X_n\) es la muestra capturada la segunda vez, entonces
\(\widehat{p}_X=\frac{k}{n}\).
\end{frame}

\begin{frame}{Estimadores máximo verosímiles. Ejemplo:
captura-recaptura}
\phantomsection\label{estimadores-muxe1ximo-verosuxedmiles.-ejemplo-captura-recaptura-1}
\(\widehat{p}_X\) es un estimador máximo verosímil \(p\). Por tanto,
estimamos que: \[
\dfrac{K}{N}=\dfrac{k}{n}\Rightarrow N=\frac{n\cdot K}{k}
\]

Por lo tanto, el estimador \[
\widehat{N}=\frac{n\cdot K}{k}
\] maximiza la probabilidad de la observación \emph{\(k\) marcados de
\(n\) capturados}, por lo que \(\hat{N}\) es el \textbf{estimador máximo
verosímil} de \(N\).
\end{frame}

\begin{frame}{Estimadores máximo verosímiles. Ejemplo:
captura-recaptura}
\phantomsection\label{estimadores-muxe1ximo-verosuxedmiles.-ejemplo-captura-recaptura-2}
\textbf{Ejercicio}

Supongamos que hemos marcado 15 peces del lago, y que en una captura, de
10 peces, hay 4 marcados. ¿Cuántos peces estimamos que contiene el lago?

El número de peces estimados del lago será: \[
\widehat{N}=\frac{15\cdot 10}{4}=37.5
\] Estimamos que habrá entre 37 y 38 peces en el lago.
\end{frame}

\begin{frame}{Estimadores máximo verosímiles. Ejemplo:
captura-recaptura}
\phantomsection\label{estimadores-muxe1ximo-verosuxedmiles.-ejemplo-captura-recaptura-3}
El estimador \[
\widehat{N}=\frac{n\cdot K}{k}
\] es sesgado, con sesgo que tiende a \(0\).

El \textbf{estimador de Chapman} \[
\widehat{N}=\frac{(n+1)\cdot (K+1)}{k+1}-1,
\] es menos segado para muestras pequeñas, y no sesgado si \(K+n\geq N\)
(pero no máximo verosímil).
\end{frame}

\section{\texorpdfstring{Estimación puntual con
\texttt{R}}{Estimación puntual con R}}\label{estimaciuxf3n-puntual-con-r}

\begin{frame}[fragile]{La función \texttt{fitdistr}}
\phantomsection\label{la-funciuxf3n-fitdistr}
Para obtener estimaciones puntuales con \texttt{R} hay que usar la
función \texttt{fitdistr} del paquete \textbf{MASS}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fitdistr}\NormalTok{(x, }\AttributeTok{densfun=}\NormalTok{..., }\AttributeTok{start=}\NormalTok{...)}
\end{Highlighting}
\end{Shaded}

donde

\begin{itemize}
\item
  \texttt{x} es la muestra, un vector numérico.
\item
  El valor de \texttt{densfun} ha de ser el nombre de la familia de
  distribuciones:\texttt{"chi-squared"}, \texttt{"exponential"},
  \texttt{"f"}, \texttt{"geometric"}, \texttt{"lognormal"},
  \texttt{"normal"} y \texttt{"poisson"}.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{La función \texttt{fitdistr}}
\phantomsection\label{la-funciuxf3n-fitdistr-1}
\begin{itemize}
\tightlist
\item
  Si \texttt{fitdistr} no dispone de una fórmula cerrada para el
  estimador máximo verosímil de algún parámetro, usa un algoritmo
  numérico para aproximarlo que requiere de un valor inicial para
  arrancar. Este valor (o valores) se puede especificar igualando el
  parámetro \texttt{start} a una \texttt{list} con cada parámetro a
  estimar igualado a un valor inicial.
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Ejemplos de uso de \texttt{fitdistr}. Estimación
del parámetro \(\lambda\) de una variable de Poisson}
\phantomsection\label{ejemplos-de-uso-de-fitdistr.-estimaciuxf3n-del-paruxe1metro-lambda-de-una-variable-de-poisson}
\textbf{Ejemplo}

Consideramos la muestra siguiente de tamaño 50 de una variable de
Poisson de parámetro \(\lambda =5\):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{98}\NormalTok{)}
\NormalTok{muestra.poisson }\OtherTok{=} \FunctionTok{rpois}\NormalTok{(}\DecValTok{50}\NormalTok{,}\AttributeTok{lambda=}\DecValTok{5}\NormalTok{)}
\NormalTok{muestra.poisson}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##  [1]  5  4  4  5  3  4  1  4  6  3  7  7  3  5  4  8  4  4  6  3  6  4  6 11  4
## [26]  7  5  2  8  3  5  4  1  5  6  4  7  7  3  4  6 10  5  4  2  9  1  5  2  2
\end{verbatim}

Vamos a estimar el valor del parámetro \(\lambda\) a partir de la
muestra anterior.
\end{frame}

\begin{frame}[fragile]{Ejemplos de uso de \texttt{fitdistr}}
\phantomsection\label{ejemplos-de-uso-de-fitdistr}
Para estimar \(\lambda\) usamos la función \texttt{fitdistr}:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(MASS)}
\FunctionTok{fitdistr}\NormalTok{(muestra.poisson, }\AttributeTok{densfun =} \StringTok{"poisson"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##     lambda 
##   4.760000 
##  (0.308545)
\end{verbatim}

La función \texttt{fitdistr} nos ha dado el siguiente valor de
\(\lambda\): 4.76, valor que se aproxima al valor real de
\(\lambda =5\), con un error típico de 0.308545.
\end{frame}

\begin{frame}[fragile]{Ejemplos de uso de \texttt{fitdistr}}
\phantomsection\label{ejemplos-de-uso-de-fitdistr-1}
Recordemos que el estimador máximo verosímil de \(\lambda\) es
\(\overline{X}\) con error típico \(\frac{\sqrt{\lambda}}{\sqrt{n}}\).
Veamos si la función \texttt{fitdistr} nos ha mentido:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(estimación}\AttributeTok{.lambda =} \FunctionTok{mean}\NormalTok{(muestra.poisson))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 4.76
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(estimación.error.típico}\OtherTok{=} \FunctionTok{sqrt}\NormalTok{(estimación.lambda}\SpecialCharTok{/}\DecValTok{50}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0.308545
\end{verbatim}

Comprobamos que los valores anteriores coinciden con los dados por la
función.
\end{frame}

\begin{frame}[fragile]{Ejemplos de uso de \texttt{fitdistr}}
\phantomsection\label{ejemplos-de-uso-de-fitdistr-2}
¿Qué estimaciones hubiésemos obtenido de la media \(\mu\) y la
desviación típica \(\sigma\) si suponemos que la muestra anterior es
normal?

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{fitdistr}\NormalTok{(muestra.poisson,}\AttributeTok{densfun =} \StringTok{"normal"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##      mean         sd    
##   4.7600000   2.1868699 
##  (0.3092701) (0.2186870)
\end{verbatim}

Dichos valores coinciden con la media muestral \(\overline{X}\) y la
desviación típica ``verdadera'' de la muestra considerada:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{(muestra.poisson)}\SpecialCharTok{*}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{49}\SpecialCharTok{/}\DecValTok{50}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 2.18687
\end{verbatim}
\end{frame}

\begin{frame}[fragile]{Guía rápida}
\phantomsection\label{guuxeda-ruxe1pida}
\begin{itemize}
\tightlist
\item
  \texttt{fitdistr} del paquete \textbf{MASS}, sirve para calcular los
  estimadores máximo verosímiles de los parámetros de una distribución a
  partir de una muestra. Parámetros principales:

  \begin{itemize}
  \tightlist
  \item
    \texttt{densfun}: el nombre de la familia de distribuciones, entre
    comillas.
  \item
    \texttt{start}: permite fijar el valor inicial del algoritmo
    numérico para calcular el estimador, si la función lo requiere.
  \end{itemize}
\end{itemize}
\end{frame}

\begin{frame}[fragile]{Estimación puntual en \texttt{Python}}
\phantomsection\label{estimaciuxf3n-puntual-en-python}
Primero haremos la carga de paquetes

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ norm}
\ImportTok{from}\NormalTok{ numpy }\ImportTok{import}\NormalTok{ linspace}
\ImportTok{import}\NormalTok{ matplotlib.pyplot }\ImportTok{as}\NormalTok{ plt}
\end{Highlighting}
\end{Shaded}

Elegimos una m.a.s. de tamaño 150 de una variable aleatoria \(N(0, 1)\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample }\OperatorTok{=}\NormalTok{ norm.rvs(loc }\OperatorTok{=} \DecValTok{0}\NormalTok{, scale }\OperatorTok{=} \DecValTok{1}\NormalTok{, size }\OperatorTok{=} \DecValTok{150}\NormalTok{)}
\BuiltInTok{print}\NormalTok{(sample[}\DecValTok{1}\NormalTok{:}\DecValTok{10}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [ 0.62046622 -1.27105933 -0.21009385 -0.6178663  -0.63405109 -1.01892704
##  -1.33981841  0.4176008  -0.43855886]
\end{verbatim}
\end{frame}

\begin{frame}[fragile]{Estimación puntual en \texttt{Python}}
\phantomsection\label{estimaciuxf3n-puntual-en-python-1}
Para estimar los parámetros, usamos la distribución adecuada (en nuestro
caso una normal) e invocamos el método \texttt{fit}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{params }\OperatorTok{=}\NormalTok{ norm.fit(sample)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Media = }\SpecialCharTok{\{mu\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(mu}\OperatorTok{=}\NormalTok{params[}\DecValTok{0}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Media = -0.045242445858551895
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\BuiltInTok{print}\NormalTok{(}\StringTok{"Desviacion tipica = }\SpecialCharTok{\{sd\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(sd}\OperatorTok{=}\NormalTok{params[}\DecValTok{1}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Desviacion tipica = 0.9409644928289411
\end{verbatim}
\end{frame}

\begin{frame}[fragile]{Estimación puntual en \texttt{Python}}
\phantomsection\label{estimaciuxf3n-puntual-en-python-2}
Vamos a comparar la distribución con los parámetros estimados vs nuestra
muestra.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ linspace(}\OperatorTok{{-}}\DecValTok{5}\NormalTok{,}\DecValTok{5}\NormalTok{,}\DecValTok{100}\NormalTok{)}
\NormalTok{pdf\_fitted }\OperatorTok{=}\NormalTok{ norm.pdf(x, loc}\OperatorTok{=}\NormalTok{params[}\DecValTok{0}\NormalTok{], scale}\OperatorTok{=}\NormalTok{params[}\DecValTok{1}\NormalTok{])}
\NormalTok{pdf\_original }\OperatorTok{=}\NormalTok{ norm.pdf(x, loc}\OperatorTok{=}\DecValTok{0}\NormalTok{, scale}\OperatorTok{=}\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\end{frame}

\begin{frame}{Estimación puntual en \texttt{Python}}
\phantomsection\label{estimaciuxf3n-puntual-en-python-3}
\pandocbounded{\includegraphics[keepaspectratio]{Tema-2---Estimacion_files/figure-beamer/unnamed-chunk-27-1.pdf}}
\end{frame}

\begin{frame}[fragile]{Ejercicio de distribución Rayleigh}
\phantomsection\label{ejercicio-de-distribuciuxf3n-rayleigh}
\[f(x) = \frac{(x-\mu)e^{-\frac{(x-\mu)^2}{2\sigma^2}}}{\sigma^2}\]

Importar las librerías para crear la m.a.s., generar la muestra y
obtener los parámetros

\begin{Shaded}
\begin{Highlighting}[]
\ImportTok{from}\NormalTok{ scipy.stats }\ImportTok{import}\NormalTok{ rayleigh}
\NormalTok{sample }\OperatorTok{=}\NormalTok{ rayleigh.rvs(loc}\OperatorTok{=}\DecValTok{5}\NormalTok{, scale}\OperatorTok{=}\DecValTok{2}\NormalTok{, size}\OperatorTok{=}\DecValTok{150}\NormalTok{)}
\NormalTok{params }\OperatorTok{=}\NormalTok{ rayleigh.fit(sample)}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Media = }\SpecialCharTok{\{mu\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(mu}\OperatorTok{=}\NormalTok{params[}\DecValTok{0}\NormalTok{]))}
\BuiltInTok{print}\NormalTok{(}\StringTok{"Desviacion tipica = }\SpecialCharTok{\{sd\}}\StringTok{"}\NormalTok{.}\BuiltInTok{format}\NormalTok{(sd}\OperatorTok{=}\NormalTok{params[}\DecValTok{1}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}
\end{frame}

\begin{frame}[fragile]{Ejercicio de distribución Rayleigh}
\phantomsection\label{ejercicio-de-distribuciuxf3n-rayleigh-1}
Generamos la distribución con los parámetros estimados

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x }\OperatorTok{=}\NormalTok{ linspace(}\DecValTok{5}\NormalTok{, }\DecValTok{15}\NormalTok{, }\DecValTok{100}\NormalTok{)}
\NormalTok{pdf\_fitted }\OperatorTok{=}\NormalTok{ rayleigh.pdf(x, loc}\OperatorTok{=}\NormalTok{params[}\DecValTok{0}\NormalTok{], scale}\OperatorTok{=}\NormalTok{params[}\DecValTok{1}\NormalTok{])}
\NormalTok{pdf\_original }\OperatorTok{=}\NormalTok{ rayleigh.pdf(x, loc}\OperatorTok{=}\DecValTok{5}\NormalTok{, scale}\OperatorTok{=}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\end{frame}

\begin{frame}{Ejercicio de distribución Rayleigh}
\phantomsection\label{ejercicio-de-distribuciuxf3n-rayleigh-2}
\pandocbounded{\includegraphics[keepaspectratio]{Tema-2---Estimacion_files/figure-beamer/unnamed-chunk-30-3.pdf}}
\end{frame}
\end{frame}

\end{document}
